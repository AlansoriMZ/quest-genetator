{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mollusca adalah hewan yang bersifat tripoblastik slomata\n",
    "# ulat mengalami perubahan bentuk\n",
    "# ulat berubah menjadi kepompong\n",
    "# kepompong berubah menjadi kupu-kupu\n",
    "# kelenjar sutra digunakan membuat kepompong\n",
    "# kupu-kupu mengalami metamorfosis sempurna karena mengalami perubahan bentuk\n",
    "# organisasi pergerakan mendorong keinginan untuk bekerja sama\n",
    "# masa praaksara adalah masa ketika manusia belum mengenal tulisan\n",
    "# tokoh organisasi berjuang bersama untuk mencapai kemerdekaan\n",
    "# ras melanesoid merupakan ras yang datang  ke kepulauan indonesia setelah proto melayu datang\n",
    "# Indonesia dijajah oleh Belanda\n",
    "\n",
    "\n",
    "# mollusca adalah hewan yang bertubuh lunak\n",
    "# Hewan > Reptil : kadal - ular - buaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from numpy import asarray\n",
    "from numpy import savetxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MEMBUAT PARSE TREE\n",
    "\n",
    "# arr = np.genfromtxt(\"K1.txt\", dtype=str, delimiter='##')\n",
    "# grammar1 = nltk.data.load('file:Grammar(ind).cfg')\n",
    "# parser = nltk.ChartParser(grammar1)\n",
    "# final = ''\n",
    "\n",
    "# for i in range(len(arr)):\n",
    "#   x = arr[i].split()\n",
    "#   x = np.array(x)\n",
    "#   output = '' \n",
    "#   for j, tree in enumerate(parser.parse(x)):\n",
    "#       # print(\"parsing grammar...\")\n",
    "#       # print(tree)\n",
    "#       output = output + str(tree)\n",
    "#       # output = output.replace(\"\\n\", \"\")\n",
    "#       # final = final + '\\n' + output\n",
    "#       final += output\n",
    "#       final += \"\\n\"\n",
    "\n",
    "# with open('K2.txt', 'w') as f:\n",
    "#    f.write(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEMBUAT PARSE TREE\n",
    "\n",
    "# Load the data\n",
    "# arr = np.genfromtxt(\"K1.txt\", dtype=str, delimiter='##')\n",
    "with open('K1.txt', 'r') as file:\n",
    "    arr = file.readlines()\n",
    "\n",
    "# Load the grammar\n",
    "grammar1 = nltk.data.load('file:Grammar(ind).cfg')\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "final = ''\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "for i in range(len(arr)):\n",
    "    x = arr[i]\n",
    "    x = remove_punctuation(x).split()  # Remove punctuation and split into words\n",
    "    x = np.array(x)\n",
    "    output = ''\n",
    "    for j, tree in enumerate(parser.parse(x)):\n",
    "        output += str(tree)\n",
    "        output += \"\\n\"\n",
    "    final += output\n",
    "\n",
    "# Write the final output to a file\n",
    "with open('K2.txt', 'w') as f:\n",
    "    f.write(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proses selesai\n"
     ]
    }
   ],
   "source": [
    "#Preproses untuk menghilangkan tanda () dll\n",
    "#K3 = K4.CSV\n",
    "import re\n",
    "\n",
    "inputs = open(r\"K2.txt\",\"r\").read()\n",
    "inputs = inputs.lower()\n",
    "inputs = inputs.split(\"\\n\")\n",
    "\n",
    "outputs1 = open(\"K3.txt\",\"w\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in inputs:\n",
    "    x = i.replace('\"', \" \")\n",
    "    a = x.replace(\"(\", \" \")\n",
    "    b = a.replace(\")\", \" \")\n",
    "    c = re.sub('\\s+',' ',b)\n",
    "\n",
    "    if(c == \" \"):\n",
    "        continue\n",
    "    \n",
    "    if (c == ' s'):\n",
    "        if(count != 0):\n",
    "            outputs1.writelines('\\n')\n",
    "        outputs1.writelines (c.strip())        \n",
    "    else:\n",
    "        outputs1.writelines (c)\n",
    "    count += 1\n",
    "outputs1.close()\n",
    "print(\"proses selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PENOMORAN KOMPONEN PARSETREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1666250678260,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "vu_poxX3CaQF"
   },
   "outputs": [],
   "source": [
    "s_tree = ['sub', 'pre', 'obj', 'pel', 'ket', 's']\n",
    "cc_tree = ['cc']\n",
    "sc_tree = ['sc']\n",
    "klausa_tree = ['klausa']\n",
    "sub_tree = ['prp', 'nn', 'nnp', 'fnum', 's', 'nn', 'pewatas']\n",
    "pre_tree = ['vb', 'fverb', 'fnom', 'fprep', 'fadje']\n",
    "obj_tree = ['nn', 'nnp', 'fnom', 'sc', 's']\n",
    "pel_tree = ['nn', 'jj', 'fnom', 'fadje', 'in', 'nnp', 'kr', 'pre', 'fverb']\n",
    "ket_tree = ['fprep', 'fket', 'fnom', 'fverb', 'sc', 's']\n",
    "pewatas_tree = ['dt', 'fnom', 'dt', 'fverb']\n",
    "fnum_tree = ['cd', 'nnd', 'cd', 'nn', 'fnum', 'prp']\n",
    "fnom_tree = ['nn', 'nnp', 'pr', 'prp', 'jj', 'fnom']\n",
    "fverb_tree = ['sc', 'vb', 'md', 'rb', 'jj', 'nnp', 'cc']\n",
    "fadje_tree = ['jj', 'rb', 'nn', 'vb', 'neg']\n",
    "bfprep_tree = ['in', 'fprep', 'nn', 'nnp', 'cd', 'fnom', 'fverb', 'fnum', 'sc']\n",
    "fket_tree = ['ketn', 'pr']\n",
    "ketn_tree = ['kemarin', 'siang', 'siang', 'sore', 'pagi', 'malam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1666250682030,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "yPiPJ_O5CbNO"
   },
   "outputs": [],
   "source": [
    "tree_dict = {\n",
    "    's': s_tree,\n",
    "    'klausa': klausa_tree,\n",
    "    'sub': sub_tree,\n",
    "    'pre': pre_tree,\n",
    "    'obj': obj_tree,\n",
    "    'pel': pel_tree,\n",
    "    'ket': ket_tree,\n",
    "    'cc': cc_tree,\n",
    "    'sc': sc_tree,\n",
    "    'pewatas': pewatas_tree,\n",
    "    'fnum': fnum_tree,\n",
    "    'fnom': fnom_tree,\n",
    "    'fverb': fverb_tree,\n",
    "    'fadje': fadje_tree,\n",
    "    'fprep': bfprep_tree,\n",
    "    'fket': fket_tree,\n",
    "    'ketn': ketn_tree\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1666250690712,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "C0ASs5qcCeQA"
   },
   "outputs": [],
   "source": [
    "#harus diset dulu jika ingin memasukkan nilai baru agar bersifat unik\n",
    "\n",
    "counter_tree = {\n",
    "    's': 0,\n",
    "    'klausa': 0,\n",
    "    'sub': 0,\n",
    "    'pre': 0,\n",
    "    'obj': 0,\n",
    "    'pel': 0,\n",
    "    'ket': 0,\n",
    "    'cc' : 0,\n",
    "    'sc' : 0,\n",
    "    'pewatas': 0,\n",
    "    'fnum': 0,\n",
    "    'fnom': 0,\n",
    "    'fverb': 0,\n",
    "    'fadje': 0,\n",
    "    'fprep': 0,\n",
    "    'fket': 0,\n",
    "    'ketn': 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K4.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy2(\"K3.txt\", \"K4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1666250710125,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "prZ4WeKeCh6x"
   },
   "outputs": [],
   "source": [
    "#remove ()dll\n",
    "def load_csv(file):\n",
    "  with open(file) as file_name:\n",
    "    file_read = csv.reader(file_name)\n",
    "    temp = list(file_read)\n",
    "    temp1 = [i[0] for i in temp]\n",
    "    final_list = []\n",
    "    for elem in temp1:\n",
    "      if '(' in elem:\n",
    "        elem = elem.replace('(', '')\n",
    "      if ')' in elem:\n",
    "        elem = elem.replace(')', '')\n",
    "      final_list.append(elem)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666250711616,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "ME4UsOhoCi6w"
   },
   "outputs": [],
   "source": [
    "arr = load_csv('K4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666250713973,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "oSKxxpcjCsht",
    "outputId": "55a7b9a9-46d5-4bdd-a60f-82960ce46c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selesai\n"
     ]
    }
   ],
   "source": [
    "#menyimpan array asli\n",
    "arr_asli = []\n",
    "\n",
    "for i in arr:\n",
    "    arr_asli.append(i.split())\n",
    "\n",
    "# for a in arr_asli:\n",
    "#   print(a)\n",
    "print(\"selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1666250732597,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "4cXpkV-CC3jl",
    "outputId": "b2aeb27b-ff6c-4041-a40c-4c66198cc901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1', 'sub1', 'nn', 'stroke', 'pre1', 'vb', 'merupakan', 'obj1', 'fadje1', 'nn', 'penyakit', 'jj', 'kronis']\n",
      "['s2', 'sub2', 'fnom1', 'nn', 'tekanan', 'fnom2', 'nn', 'darah', 'nnp', 'tinggi', 'pre2', 'fverb1', 'md', 'dapat', 'vb', 'menyebabkan', 'obj2', 'nn', 'kematian']\n",
      "['s3', 'sub3', 'fnom3', 'nn', 'tekanan', 'fnom4', 'nn', 'darah', 'nn', 'tinggi', 'pre3', 'fverb2', 'md', 'dapat', 'vb', 'menyebabkan', 'obj3', 'nn', 'kematian']\n",
      "selesai\n"
     ]
    }
   ],
   "source": [
    "#menyimpan array dengan number\n",
    "arr_gabungan = []\n",
    "for i in arr_asli:\n",
    "  temp = []\n",
    "  for j in range(len(i)):\n",
    "    if i[j] in tree_dict:\n",
    "      curr = i[j]\n",
    "      counter_tree[curr] += 1\n",
    "      temp.append(curr+str(counter_tree[curr]))\n",
    "    else:\n",
    "      temp.append(i[j])\n",
    "  arr_gabungan.append(temp)\n",
    "\n",
    "for a in arr_gabungan:\n",
    "  print(a)\n",
    "print(\"selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s': 3,\n",
       " 'klausa': 0,\n",
       " 'sub': 3,\n",
       " 'pre': 3,\n",
       " 'obj': 3,\n",
       " 'pel': 0,\n",
       " 'ket': 0,\n",
       " 'cc': 0,\n",
       " 'sc': 0,\n",
       " 'pewatas': 0,\n",
       " 'fnum': 0,\n",
       " 'fnom': 4,\n",
       " 'fverb': 2,\n",
       " 'fadje': 1,\n",
       " 'fprep': 0,\n",
       " 'fket': 0,\n",
       " 'ketn': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1666250791346,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "2aFeC0wHDD-b"
   },
   "outputs": [],
   "source": [
    "#cari letak komponen tree\n",
    "\n",
    "cari = ['s','a_sub','b_pre','c_obj','d_pel','e_ket','cc','sc','fnom','fnum','fprep','fket','fverb','fadje','pewatas','konj','subord','klausa']\n",
    "def find(arr):\n",
    "  x = np.array(arr)\n",
    "  ls = np.where(x == 's')\n",
    "  lsub = np.where(x == 'a_sub')\n",
    "  lpre = np.where(x == 'b_pre')\n",
    "  lobj = np.where(x == 'c_obj')\n",
    "  lpel = np.where(x == 'd_pel')\n",
    "  lket = np.where(x == 'e_ket')\n",
    "  lcc = np.where(x == 'cc')\n",
    "  lsc = np.where(x == 'sc')\n",
    "  lfnom = np.where(x == 'fnom')\n",
    "  lfnum = np.where(x == 'fnum')\n",
    "  lfprep = np.where(x == 'fprep')\n",
    "  lfket = np.where(x == 'fket')\n",
    "  lfverb = np.where(x == 'fverb')\n",
    "  lfadje = np.where(x == 'fadje')\n",
    "  lpewatas = np.where(x == 'pewatas')\n",
    "  lkonj = np.where(x == 'konj')\n",
    "  lsubord = np.where(x == 'subord')\n",
    "  lklausa = np.where(x == 'klausa')\n",
    "\n",
    "  ls_ = ls[0].tolist()\n",
    "  lsub_ = lsub[0].tolist()\n",
    "  lpre_ = lpre[0].tolist()\n",
    "  lobj_ = lobj[0].tolist()\n",
    "  lpel_ = lpel[0].tolist()\n",
    "  lket_ = lket[0].tolist()\n",
    "  lcc_ = lcc[0].tolist()\n",
    "  lsc_ = lsc[0].tolist()\n",
    "  lfnom_ = lfnom[0].tolist()\n",
    "  lfnum_ = lfnum[0].tolist()\n",
    "  lfprep_ = lfprep[0].tolist()\n",
    "  lfket_ = lfket[0].tolist()\n",
    "  lfverb_ = lfverb[0].tolist()\n",
    "  lfadje_ = lfadje[0].tolist()\n",
    "  lpewatas_ = lpewatas[0].tolist()\n",
    "  lkonj_ = lkonj[0].tolist()\n",
    "  lsubord_ = lsubord[0].tolist()\n",
    "  lklausa_ = lklausa[0].tolist()\n",
    "\n",
    "  return {\n",
    "      's': ls_,\n",
    "      'a_sub': lsub_,\n",
    "      'b_pre': lpre_,\n",
    "      'c_obj': lobj_,\n",
    "      'd_pel': lpel_,\n",
    "      'e_ket': lket_,\n",
    "      'cc' : lcc_,\n",
    "      'sc' : lsc_,\n",
    "      'fnom': lfnom_,\n",
    "      'fnum': lfnum_,\n",
    "      'fprep': lfprep_,\n",
    "      'fket': lfket_,\n",
    "      'fverb': lfverb_,\n",
    "      'fadje': lfadje_,\n",
    "      'pewatas': lpewatas_,\n",
    "      'konj': lkonj_,\n",
    "      'subord': lsubord_,\n",
    "      'klausa': lklausa_\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1666250794059,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "qN4YKZf_DItC",
    "outputId": "e9efbced-1b5e-4bd2-d026-63cc7809f05e"
   },
   "outputs": [],
   "source": [
    "#lihat lokasi komponen\n",
    "# arr_loc = []\n",
    "# for i in arr_asli:\n",
    "#   dt = find(i)\n",
    "#   arr_loc.append(dt)\n",
    "\n",
    "# for l in arr_loc:\n",
    "#    print(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEMBUATAN TRIPLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tahap 1 selesai\n"
     ]
    }
   ],
   "source": [
    "# #mendapatkan type SPOK\n",
    "file1 = open(\"K5.txt\",\"w\")\n",
    "\n",
    "for arr_a in range(len(arr_asli)):\n",
    "    for i in range(len(arr_asli[arr_a])-1):\n",
    "        tree_asli = arr_asli[arr_a][i]\n",
    "        next_tree_asli = arr_asli[arr_a][i + 1]\n",
    "        tree_g = arr_gabungan[arr_a][i]\n",
    "        next_tree_g = arr_gabungan[arr_a][i + 1]\n",
    "\n",
    "        if tree_asli == 's': \n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'sub': \n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n') \n",
    "        if tree_asli == 'pre': \n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'obj': \n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'pel': \n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n') \n",
    "        if tree_asli == 'ket': \n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n') \n",
    "        if tree_asli == 'fnom': \n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'fprep': \n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'fverb':\n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'pewatas':\n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'klausa':\n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'fnum':\n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "        if tree_asli == 'fadje':\n",
    "            file1.writelines(f'{tree_g} type {tree_asli}\\n')\n",
    "\n",
    "\n",
    "print(\"tahap 1 selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tahap 2 selesai\n"
     ]
    }
   ],
   "source": [
    "# #mendapatkan type dari Tagset\n",
    "\n",
    "for arr_a in range(len(arr_asli)):\n",
    "    for i in range(len(arr_asli[arr_a])-1):\n",
    "        tree_asli = arr_asli[arr_a][i]\n",
    "        next_tree_asli = arr_asli[arr_a][i + 1]\n",
    "        tree_g = arr_gabungan[arr_a][i]\n",
    "        next_tree_g = arr_gabungan[arr_a][i + 1]\n",
    "\n",
    "        if tree_asli == 'nn': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'sc': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'vb': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'nnp': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'jj': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'rb': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'in': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'cd': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'cc': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'pr': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'prp': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'md': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'fw': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'neg': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'dt': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'nnd': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'sym': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'rp': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'OD': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'x': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'wh': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "        if tree_asli == 'uh': \n",
    "            file1.writelines(f'{arr_gabungan[arr_a][i+1]} type {arr_asli[arr_a][i]} \\n')\n",
    "\n",
    "\n",
    "\n",
    "print(\"tahap 2 selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1666254201704,
     "user": {
      "displayName": "Selvia FK",
      "userId": "15526762061037364658"
     },
     "user_tz": -420
    },
    "id": "8eiTkJJmQuZZ",
    "outputId": "0a17e495-4358-4a14-8ed6-620660636105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tahap 3 selesai\n"
     ]
    }
   ],
   "source": [
    "#pembuatan triplet (versi 3)\n",
    "#konsepnya mundur, s-p-o-pel-k\n",
    "#file1 = open(\"TripletKalimat.txt\",\"w\")\n",
    "\n",
    "for arr_a in range(len(arr_asli)):\n",
    "  #print()\n",
    "  for i in range(len(arr_asli[arr_a])-1):\n",
    "    tree_asli = arr_asli[arr_a][i]\n",
    "    next_tree_asli = arr_asli[arr_a][i + 1]\n",
    "    tree_g = arr_gabungan[arr_a][i]\n",
    "    next_tree_g = arr_gabungan[arr_a][i + 1]\n",
    "  \n",
    "    try:\n",
    "        if (tree_asli == 'sub'and next_tree_asli != 'fnom'):\n",
    "            if (next_tree_asli != 'fnum'):\n",
    "                file1.writelines(f'{tree_g} has{next_tree_asli} {arr_gabungan[arr_a][i+2]}\\n') # sub has nnp\n",
    "        if (tree_asli == 'pre'and next_tree_asli != 'fverb'):\n",
    "            file1.writelines (f'{tree_g} has{next_tree_asli} {arr_gabungan[arr_a][i+2]}\\n') # pre has vb\n",
    "        if (tree_asli == 'obj'and next_tree_asli != 'fnom'):\n",
    "            file1.writelines (f'{tree_g} has{next_tree_asli} {arr_gabungan[arr_a][i+2]}\\n') # obj has nnp\n",
    "        if (tree_asli == 'pel'and next_tree_asli != 'fnom'):\n",
    "            file1.writelines (f'{tree_g} has{next_tree_asli} {arr_gabungan[arr_a][i+2]}\\n') # pel has nnp\n",
    "        if (tree_asli == 'ket' and  arr_asli[arr_a][i + 1] == 'sc'):\n",
    "            file1.writelines (f'{tree_g} hasa{arr_asli[arr_a][i + 1]} {arr_asli[arr_a][i + 2]}\\n') # ket hasSC x\n",
    "            if (arr_asli[arr_a][i + 3] == 'vb' or arr_asli[arr_a][i + 3] == 'nn' or arr_asli[arr_a][i + 3] == 'nnp'):\n",
    "                file1.writelines (f'{tree_g} has{arr_asli[arr_a][i + 3]} {arr_asli[arr_a][i + 4]}\\n') # ket hasVB x\n",
    "    except:\n",
    "        file1.writelines('\\n')\n",
    "            \n",
    "    try:\n",
    "        if  tree_asli == 'fnom':\n",
    "            \n",
    "            if (arr_asli[arr_a][i + 1] != 'fnom' and (arr_asli[arr_a][i + 3] == 'fprep' or arr_asli[arr_a][i + 3] == 'fnom')): \n",
    "                file1.writelines (f'{tree_g} has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]}\\n')#fnom nn fnom\n",
    "            if (arr_asli[arr_a][i + 1] != 'fnom' and arr_asli[arr_a][i + 1] != 'fadje' and arr_asli[arr_a][i + 1] != 'fnum' and arr_asli[arr_a][i + 3] != 'fnom'):  #fnom normal\n",
    "                file1.writelines (f'{tree_g} hasfnom {tree_g}a \\n')\n",
    "                file1.writelines (f'{tree_g}a has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')\n",
    "                file1.writelines (f'{tree_g} hasfnom {tree_g}b \\n')\n",
    "                file1.writelines (f'{tree_g}b has{arr_asli[arr_a][i + 3]} {arr_gabungan[arr_a][i + 4]} \\n')\n",
    "                file1.writelines (f'{tree_g} type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}a type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}b type {tree_asli} \\n')\n",
    "            if (arr_asli[arr_a][i + 1] != 'fnom' and arr_asli[arr_a][i + 3] == 'cc'):  #fnom cc\n",
    "                file1.writelines (f'{tree_g} hasfnom {tree_g}a \\n')\n",
    "                file1.writelines (f'{tree_g}a has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')\n",
    "                file1.writelines (f'{tree_g} hasfnom {tree_g}b \\n')\n",
    "                file1.writelines (f'{tree_g}b has{arr_asli[arr_a][i + 3]} {arr_gabungan[arr_a][i + 4]} \\n')\n",
    "                file1.writelines (f'{tree_g} hasfnom {tree_g}c \\n')\n",
    "                file1.writelines (f'{tree_g}c has{arr_asli[arr_a][i + 5]} {arr_gabungan[arr_a][i + 6]} \\n')\n",
    "                file1.writelines (f'{tree_g}  type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}a type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}b type {tree_asli} \\n')   \n",
    "                file1.writelines (f'{tree_g}c type {tree_asli} \\n')\n",
    "    except:\n",
    "        file1.writelines('\\n')\n",
    "    \n",
    "    try:\n",
    "        if  tree_asli == 'fnum':\n",
    "            if (arr_asli[arr_a][i + 1] != 'fnum' and (arr_asli[arr_a][i + 3] == 'fprep' or \n",
    "                arr_asli[arr_a][i + 3] == 'fnom')): \n",
    "                file1.writelines (f'{tree_g} has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]}\\n')#fprep nn fprep\n",
    "            if (arr_asli[arr_a][i + 1] != 'fnom' and arr_asli[arr_a][i + 1] != 'fnum' and \n",
    "                arr_asli[arr_a][i + 3] != 'fnom'  and arr_asli[arr_a][i + 3] != 'fprep'):  #fnum normal\n",
    "                file1.writelines (f'{tree_g} hasfnum {tree_g}a \\n')\n",
    "                file1.writelines (f'{tree_g}a has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')\n",
    "                file1.writelines (f'{tree_g} hasfnum {tree_g}b \\n')\n",
    "                file1.writelines (f'{tree_g}b has{arr_asli[arr_a][i + 3]} {arr_gabungan[arr_a][i + 4]} \\n')\n",
    "                file1.writelines (f'{tree_g} type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}a type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}b type {tree_asli} \\n') \n",
    "    except:\n",
    "        file1.writelines('\\n')\n",
    "    \n",
    "    try:\n",
    "        if  tree_asli == 'fverb': #fverb normal\n",
    "                if arr_asli[arr_a][i + 3] == 'fverb':\n",
    "                    file1.writelines (f'{tree_g} has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]}\\n')\n",
    "                if (arr_asli[arr_a][i + 1] != 'fverb' and arr_asli[arr_a][i + 3] == 'cc'):  #fnom cc\n",
    "                    file1.writelines (f'{tree_g} hasfnom {tree_g}a \\n')\n",
    "                    file1.writelines (f'{tree_g}a has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')\n",
    "                    file1.writelines (f'{tree_g} hasfnom {tree_g}b \\n')\n",
    "                    file1.writelines (f'{tree_g}b has{arr_asli[arr_a][i + 3]} {arr_gabungan[arr_a][i + 4]}\\n')\n",
    "                    file1.writelines (f'{tree_g} hasfnom {tree_g}c \\n')\n",
    "                    file1.writelines (f'{tree_g}c has{arr_asli[arr_a][i + 5]} {arr_gabungan[arr_a][i + 6]} \\n')\n",
    "                    file1.writelines (f'{tree_g}  type {tree_asli} \\n')\n",
    "                    file1.writelines (f'{tree_g}a type {tree_asli} \\n')\n",
    "                    file1.writelines (f'{tree_g}b type {tree_asli} \\n')   \n",
    "                    file1.writelines (f'{tree_g}c type {tree_asli} \\n')\n",
    "                else:  \n",
    "                    file1.writelines (f'{tree_g} type {tree_asli}\\n')\n",
    "                    file1.writelines (f'{tree_g}a type {tree_asli} \\n')\n",
    "                    file1.writelines (f'{tree_g}b type {tree_asli} \\n')\n",
    "                    file1.writelines (f'{tree_g} hasfverb {tree_g}a \\n')\n",
    "                    file1.writelines (f'{tree_g}a has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')\n",
    "                    file1.writelines (f'{tree_g} hasfverb {tree_g}b \\n')\n",
    "                    file1.writelines (f'{tree_g}b has{arr_asli[arr_a][i + 3]} {arr_gabungan[arr_a][i + 4]} \\n')\n",
    "    except:\n",
    "        file1.writelines('\\n')\n",
    "                \n",
    "    try:\n",
    "        if  tree_asli == 'fprep':\n",
    "            if (arr_asli[arr_a][i + 1] != 'fprep' and (arr_asli[arr_a][i + 3] == 'fprep' or \n",
    "                arr_asli[arr_a][i + 3] == 'fnom')): \n",
    "                file1.writelines (f'{tree_g} has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')#fprep nn fprep\n",
    "            if (arr_asli[arr_a][i + 1] != 'fprep' and arr_asli[arr_a][i + 1] != 'fnum' and \n",
    "                arr_asli[arr_a][i + 3] != 'fnom'  and arr_asli[arr_a][i + 3] != 'fprep'):  #fadje normal\n",
    "                file1.writelines (f'{tree_g} hasfprep {tree_g}a \\n')\n",
    "                file1.writelines (f'{tree_g}a has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')\n",
    "                file1.writelines (f'{tree_g} hasfprep {tree_g}b \\n')\n",
    "                file1.writelines (f'{tree_g}b has{arr_asli[arr_a][i + 3]} {arr_gabungan[arr_a][i + 4]} \\n')\n",
    "                file1.writelines (f'{tree_g} type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}a type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}b type {tree_asli} \\n') \n",
    "    except:\n",
    "        file1.writelines('\\n')\n",
    "    \n",
    "    try:\n",
    "        if  tree_asli == 'fadje':\n",
    "            if (arr_asli[arr_a][i + 1] != 'fadje' and (arr_asli[arr_a][i + 3] == 'fprep' or \n",
    "                arr_asli[arr_a][i + 3] == 'fnom')): \n",
    "                file1.writelines (f'{tree_g} has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')#fadje nn fadje\n",
    "            if (arr_asli[arr_a][i + 1] != 'fadje' and arr_asli[arr_a][i + 1] != 'fnum' and \n",
    "                arr_asli[arr_a][i + 3] != 'fnom'):  #fadje normal\n",
    "                file1.writelines (f'{tree_g} hasfadje {tree_g}a \\n')\n",
    "                file1.writelines (f'{tree_g}a has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')\n",
    "                file1.writelines (f'{tree_g} hasfadje {tree_g}b  \\n')\n",
    "                file1.writelines (f'{tree_g}b has{arr_asli[arr_a][i + 3]} {arr_gabungan[arr_a][i + 4]} \\n')\n",
    "                file1.writelines (f'{tree_g} type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}a type {tree_asli} \\n')\n",
    "                file1.writelines (f'{tree_g}b type {tree_asli} \\n') \n",
    "    except:\n",
    "        file1.writelines('\\n')\n",
    "             \n",
    "    try:        \n",
    "        if  tree_asli == 'pewatas':\n",
    "            if (arr_asli[arr_a][i + 3] == 'fnom' or arr_asli[arr_a][i + 3] == 'fprep' or arr_asli[arr_a][i + 3] == 'fverb'): #Pew nn Fnom\n",
    "                file1.writelines (f'{tree_g} has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')          \n",
    "            else:\n",
    "                file1.writelines (f'{tree_g}a type pewatas \\n')\n",
    "                file1.writelines (f'{tree_g}b type pewatas \\n')\n",
    "                file1.writelines (f'{tree_g}c type pewatas \\n')  \n",
    "                file1.writelines (f'{tree_g}a has{arr_asli[arr_a][i + 1]} {arr_gabungan[arr_a][i + 2]} \\n')\n",
    "                file1.writelines (f'{tree_g} haspewatas {tree_g}a \\n')\n",
    "                file1.writelines (f'{tree_g}b has{arr_asli[arr_a][i + 3]} {arr_gabungan[arr_a][i + 4]} \\n')\n",
    "                file1.writelines (f'{tree_g} haspewatas {tree_g}b \\n') \n",
    "                file1.writelines (f'{tree_g}c has{arr_asli[arr_a][i + 5]} {arr_gabungan[arr_a][i + 6]} \\n')\n",
    "                file1.writelines (f'{tree_g} haspewatas {tree_g}c \\n') \n",
    "                file1.writelines (f'{tree_g} type {tree_asli} \\n')        \n",
    "    except:\n",
    "        file1.writelines ('\\n')\n",
    "\n",
    "#file1.close()\n",
    "print(\"tahap 3 selesai\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tahap 4 selesai\n"
     ]
    }
   ],
   "source": [
    "for arr_a in range(len(arr_asli)):\n",
    "  #print(\"\\n\")\n",
    "  for i in range(len(arr_asli[arr_a])-1):\n",
    "    tree_asli = arr_asli[arr_a][i]\n",
    "    next_tree_asli = arr_asli[arr_a][i + 1]\n",
    "    tree_g = arr_gabungan[arr_a][i]\n",
    "    next_tree_g = arr_gabungan[arr_a][i + 1]\n",
    "\n",
    "#mendapatkan susukan kalimat dari SPOK FNOM dll Pew\n",
    "    if tree_asli == 's': \n",
    "        s=tree_g\n",
    "    if tree_asli == 'klausa': \n",
    "        if (tree_asli == 'klausa' and arr_asli[arr_a][i-1]=='s'):\n",
    "            file1.writelines (f'{arr_gabungan[arr_a][0]} hasklausa {tree_g} \\n') \n",
    "            s=tree_g\n",
    "        if (tree_asli == 'klausa' and arr_asli[arr_a][i-2]=='cc'):\n",
    "             file1.writelines (f'{arr_gabungan[arr_a][0]} hasklausa {tree_g} \\n') \n",
    "             file1.writelines (f'{tree_g} hascc {arr_asli[arr_a][i-1]} \\n')\n",
    "             s=tree_g\n",
    "        if (tree_asli == 'klausa' and arr_asli[arr_a][i-2]=='sc'):\n",
    "             file1.writelines (f'{arr_gabungan[arr_a][i-3]} hasklausa {tree_g} \\n') \n",
    "             file1.writelines (f'{arr_gabungan[arr_a][i-3]} hasasc {arr_asli[arr_a][i-1]} \\n')\n",
    "             s=tree_g\n",
    "                \n",
    "    if tree_asli == 'sub': \n",
    "        file1.writelines (f'{s} hasasub {tree_g} \\n')   \n",
    "        tag = (f'{tree_g}')\n",
    "    if tree_asli == 'pre': \n",
    "        file1.writelines (f'{s} hasbpre {tree_g} \\n')\n",
    "        tag =(f'{tree_g}')\n",
    "    if tree_asli == 'obj': \n",
    "        file1.writelines (f'{s} hascobj {tree_g} \\n')\n",
    "        tag =(f'{tree_g}')\n",
    "    if tree_asli == 'pel': \n",
    "        file1.writelines (f'{s} hasdpel {tree_g} \\n')\n",
    "        tag =(f'{tree_g}')\n",
    "    if tree_asli == 'ket': \n",
    "        file1.writelines (f'{s} haseket {tree_g} \\n')\n",
    "        tag =(f'{tree_g}')\n",
    "    if tree_asli == 'pewatas': \n",
    "        file1.writelines (f'{tag} hasznext {tree_g} \\n')\n",
    "        tag=(f'{tree_g}')\n",
    "    if tree_asli == 'fnom': \n",
    "        file1.writelines (f'{tag} hasznext {tree_g} \\n')\n",
    "        tag=(f'{tree_g}')\n",
    "    if tree_asli == 'fnum': \n",
    "        file1.writelines (f'{tag} hasznext {tree_g} \\n')\n",
    "        tag=(f'{tree_g}')     \n",
    "    if tree_asli == 'fverb': \n",
    "        file1.writelines (f'{tag} hasznext {tree_g}\\n')\n",
    "        tag=(f'{tree_g}')         \n",
    "    if tree_asli == 'fprep': \n",
    "        file1.writelines (f'{tag} hasznext {tree_g} \\n')\n",
    "        tag=(f'{tree_g}')          \n",
    "    if tree_asli == 'fadje': \n",
    "        file1.writelines (f'{tag} hasznext {tree_g} \\n')\n",
    "        tag=(f'{tree_g}')\n",
    "#         if (arr_asli[arr_a][i+1] != 'fadje' and (arr_asli[arr_a][i+1] == 'fnom' or arr_asli[arr_a][i+1] == 'fnum'):\n",
    "#             file1.writelines (f'{tag} has{arr_asli[arr_a][i+1]} {arr_gabungan[arr_a][i]} \\n') \n",
    "\n",
    "        \n",
    "file1.close()\n",
    "print(\"tahap 4 selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# LOAD SELURUH TRIPLET YANG DIHASILKAN AGAR MASUK KE OWL, CARA LOAD ADA DI BAWAH\n",
    "# PADA BAGIAN ATAS INI, HILANGKAN BARIS YG TIDAK BERUPA TRIPLET (FNOM1 DAN KET1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIPLET TO OWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bacaTriplet(namaFile):\n",
    "    lines = []\n",
    "    with open(namaFile,'r+') as f:\n",
    "        line=f.read().splitlines()\n",
    "        lines.append(line)\n",
    "    lines = np.array(lines)\n",
    "    subjek = []\n",
    "    for i in range(lines.shape[1]):\n",
    "        a = lines[0,i].split(\" \")\n",
    "        subjek.append(a)\n",
    "    return subjek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet = bacaTriplet('K5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "onto = get_ontology(\"Pharmacho.owl\").load()\n",
    "# onto = get_ontology(\"http://test.org/onto.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menambahkan Instance\n",
    "\n",
    "from owlready2 import *\n",
    "onto = get_ontology(\"Pharmacho.owl\").load()\n",
    "\n",
    "for i in triplet:\n",
    "    if(i[1] == 'type' and i[2] == 's'):\n",
    "        onto.Kalimat(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'klausa'):\n",
    "        onto.Klausa(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'sub'):\n",
    "        onto.Subjek(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'pre'):\n",
    "        onto.Predikat(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'obj'):\n",
    "        onto.Objek(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'pel'):\n",
    "        onto.Pelengkap(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'ket'):\n",
    "        onto.Keterangan(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'pewatas'):\n",
    "        onto.Pewatas(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'cc'):\n",
    "        onto.CC(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'cd'):\n",
    "        onto.CD(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'dt'):\n",
    "        onto.DT(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'fadje'):\n",
    "        onto.FAdje(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'fadv'):\n",
    "        onto.FAdv(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'fnom'):\n",
    "        onto.FNom(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'fnum'):\n",
    "        onto.FNum(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'fprep'):\n",
    "        onto.FPrep(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'fverb'):\n",
    "        onto.FVerb(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'fw'):\n",
    "        onto.FW(i[0])    \n",
    "    if(i[1] == 'type' and i[2] == 'in'):\n",
    "        onto.IN(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'jj'):\n",
    "        onto.JJ(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'md'):\n",
    "        onto.MD(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'neg'):\n",
    "        onto.NEG(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'nn'):\n",
    "        onto.NN(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'nnd'):\n",
    "        onto.NND(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'nnp'):\n",
    "        onto.NNP(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'od'):\n",
    "        onto.OD(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'pr'):\n",
    "        onto.PR(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'prp'):\n",
    "        onto.PRP(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'rb'):\n",
    "        onto.RB(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'rp'):\n",
    "        onto.RP(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'sc'):\n",
    "        onto.SC(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'sym'):\n",
    "        onto.SYM(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'uh'):\n",
    "        onto.UH(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'vb'):\n",
    "        onto.VB(i[0])\n",
    "    if(i[1] == 'type' and i[2] == 'wh'):\n",
    "        onto.WH(i[0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#menyimpan owl\n",
    "onto.save(\"Pharmacho.owl\", format = \"rdfxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penambahan Relasi selesai\n"
     ]
    }
   ],
   "source": [
    "#https://pythonhosted.org/Owlready/properties.html\n",
    "#Menambahkan Relasi\n",
    "file2 = open(\"K6.txt\",\"w\")\n",
    "\n",
    "for i in triplet:   \n",
    "    if(i[1] == 'hasrb'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasRB.append(onto.RB(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasrp'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasRP.append(onto.RP(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasasc'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasaSC.append(onto.SC(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hassym'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasSYM.append(onto.SYM(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasuh'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasUH.append(onto.UH(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasvb'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasVB.append(onto.VB(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'haswh'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasWH.append(onto.WH(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasmd'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasMD.append(onto.MD(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hascc'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasCC.append(onto.CC(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hascd'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasCD.append(onto.CD(\"'+i[2]+'\"))\\n') \n",
    "    if(i[1] == 'hasdt'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasDT.append(onto.DT(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasfw'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasFW.append(onto.FW(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasin'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasIN.append(onto.IN(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasjj'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasJJ.append(onto.JJ(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasnn'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasNN.append(onto.NN(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasnnd'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasNND.append(onto.NND(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasnnp'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasNNP.append(onto.NNP(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasneg'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasNEG.append(onto.NEG(\"'+i[2]+'\"))\\n') \n",
    "    if(i[1] == 'hasod'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasOD.append(onto.OD(\"'+i[2]+'\"))\\n') \n",
    "    if(i[1] == 'haspr'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasPR.append(onto.PR(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasprp'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasPRP.append(onto.PRP(\"'+i[2]+'\"))\\n')\n",
    "        \n",
    "    if(i[1] == 'hasnext'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasNext.append(onto.S(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasprev'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasPrev.append(onto.Prev(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'haspart'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasPart.append(onto.Part(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasznext'):\n",
    "        if ('pewatas' in i[2]):\n",
    "            file2.writelines ('onto.'+i[0]+'.hasZnext.append(onto.Pewatas(\"'+i[2]+'\"))\\n')\n",
    "        if ('fadje' in i[2]):\n",
    "            file2.writelines ('onto.'+i[0]+'.hasZnext.append(onto.FAdje(\"'+i[2]+'\"))\\n')\n",
    "        if ('fnom' in i[2]):\n",
    "            file2.writelines ('onto.'+i[0]+'.hasZnext.append(onto.FNom(\"'+i[2]+'\"))\\n')\n",
    "        if ('fnum' in i[2]):\n",
    "            file2.writelines ('onto.'+i[0]+'.hasZnext.append(onto.FNum(\"'+i[2]+'\"))\\n')\n",
    "        if ('fprep' in i[2]):\n",
    "            file2.writelines ('onto.'+i[0]+'.hasZnext.append(onto.FPrep(\"'+i[2]+'\"))\\n')\n",
    "        if ('fverb' in i[2]):\n",
    "            file2.writelines ('onto.'+i[0]+'.hasZnext.append(onto.FVerb(\"'+i[2]+'\"))\\n')\n",
    "        \n",
    "    if(i[1] == 'hasfadje'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasFadje.append(onto.FAdje(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasfnom'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasFnom.append(onto.FNom(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasfnum'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasFnum.append(onto.FNum(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasfprep'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasFprep.append(onto.FPrep(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasfverb'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasFverb.append(onto.FVerb(\"'+i[2]+'\"))\\n')\n",
    "    \n",
    "    if(i[1] == 'hasklausa'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasKlausa.append(onto.Klausa(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasasub'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasaSub.append(onto.Subjek(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasbpre'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasbPre.append(onto.Predikat(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hascobj'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hascObj.append(onto.Objek(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'hasdpel'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasdPel.append(onto.Pelengkap(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'haseket'):\n",
    "        file2.writelines ('onto.'+i[0]+'.haseKet.append(onto.Keterangan(\"'+i[2]+'\"))\\n')\n",
    "    if(i[1] == 'haspewatas'):\n",
    "        file2.writelines ('onto.'+i[0]+'.hasPewatas.append(onto.Pewatas(\"'+i[2]+'\"))\\n')\n",
    "\n",
    "\n",
    "file2.close()\n",
    "print (\"Penambahan Relasi selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEMASUKKAN RELASI [S]-->[SUB] or [S]-->[PRE]\n",
    "from owlready2 import *\n",
    "onto = get_ontology(\"Pharmacho.owl\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert data from TripletRelasi.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology loaded.\n",
      "Data from K6.txt has been added and ontology has been saved.\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import get_ontology, Thing\n",
    "\n",
    "# Memuat ontologi dari file\n",
    "ontology_path = \"Pharmacho.owl\"\n",
    "ontology = get_ontology(ontology_path).load()\n",
    "print(\"Ontology loaded.\")\n",
    "\n",
    "# Membaca data dari file K6.txt\n",
    "file_path = \"K6.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Fungsi untuk memastikan entitas ada atau membuatnya jika belum ada\n",
    "def get_or_create_entity(ontology, entity_name, entity_type):\n",
    "    entity = getattr(ontology, entity_name, None)\n",
    "    if entity is None:\n",
    "        entity = type(entity_name, (entity_type,), {})\n",
    "        setattr(ontology, entity_name, entity)\n",
    "    return entity\n",
    "\n",
    "# Mengeksekusi perintah dari file K6.txt\n",
    "with ontology:\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Parsing command\n",
    "        if line.startswith(\"onto.\"):\n",
    "            try:\n",
    "                command = line.split(\".\")[1]\n",
    "                entity_name, property_command = command.split(\".\")[0], \".\".join(command.split(\".\")[1:])\n",
    "                entity_type_name = property_command.split(\"(\")[0]\n",
    "                property_name = property_command.split(\".\")[0]\n",
    "\n",
    "                # Get or create entity\n",
    "                entity_type = getattr(ontology, entity_type_name, Thing)\n",
    "                entity = get_or_create_entity(ontology, entity_name, entity_type)\n",
    "\n",
    "                # Execute the original command\n",
    "                exec(line)\n",
    "            except IndexError:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line}: {e}\")\n",
    "\n",
    "# Menyimpan ontologi yang telah diperbarui\n",
    "ontology.save(file=ontology_path)\n",
    "print(\"Data from K6.txt has been added and ontology has been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#menyimpan owl\n",
    "onto.save(\"Pharmacho.owl\", format = \"rdfxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onto.s2.hasNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tambah hasNext dan seeAlso\n",
    "# from owlready2 import *\n",
    "# onto = get_ontology(\"Eksperimen.owl\").load()\n",
    "\n",
    "# # onto.Mollusca.seeAlso.append(onto.Kalimat(\"s1\"))\n",
    "# onto.s2.hasNext.append(onto.Kalimat(\"sV3\"))\n",
    "\n",
    "# onto.save(\"Pharmacho.owl\", format = \"rdfxml\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
