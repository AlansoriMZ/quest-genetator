{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import docx\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY RELASI KALIMAT\n",
    "\n",
    "from owlready2 import *\n",
    "import rdflib\n",
    "\n",
    "onto = get_ontology(\"Pharmacho.owl\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PENGAMBILAN SUSUNAN KALIMAT OTOMATIS\n",
    "import numpy as np\n",
    "#file1 = open(\"HasilRec.txt\",\"w\")\n",
    "\n",
    "def recursion(x):\n",
    "    \n",
    "    result = list(default_world.sparql(\"\"\"\n",
    "    PREFIX okbi: <http://purl.org/net/Pharmaco#>\n",
    "    SELECT ?subjek ?predikat ?objek\n",
    "    WHERE\n",
    "    { \n",
    "      ?p rdf:type owl:ObjectProperty \n",
    "      ?predikat rdfs:subPropertyOf* ?p.\n",
    "      ?subjek ?predikat ?objek. \n",
    "      FILTER (?subjek =\"\"\" + x + \"\"\"\n",
    "      )\n",
    "    }\n",
    "    \"\"\"))\n",
    "    \n",
    "    #print(x)\n",
    "    result = np.array(result)\n",
    "    if(len(result) !=0 ):\n",
    "        res_recursion.append(result)\n",
    "        print(result)\n",
    "    \n",
    "    for i in result:\n",
    "        if(str(i[1]).split('.')[1]!='hasNext'):\n",
    "            recursion('okbi:'+str(i[2]).split('.')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Pharmacho.s1 Pharmacho.hasaSub Pharmacho.sub1]\n",
      " [Pharmacho.s1 Pharmacho.hasbPre Pharmacho.pre1]\n",
      " [Pharmacho.s1 Pharmacho.hascObj Pharmacho.obj1]]\n",
      "[[Pharmacho.sub1 Pharmacho.hasNN Pharmacho.stroke]]\n",
      "[[Pharmacho.pre1 Pharmacho.hasVB Pharmacho.merupakan]]\n",
      "[[Pharmacho.obj1 Pharmacho.hasFadje Pharmacho.nn]\n",
      " [Pharmacho.obj1 Pharmacho.hasZnext Pharmacho.fadje1]]\n",
      "[[Pharmacho.fadje1 Pharmacho.hasFadje Pharmacho.fadje1a]\n",
      " [Pharmacho.fadje1 Pharmacho.hasFadje Pharmacho.fadje1b]]\n",
      "[[Pharmacho.fadje1a Pharmacho.hasNN Pharmacho.penyakit]]\n",
      "[[Pharmacho.fadje1b Pharmacho.hasJJ Pharmacho.kronis]]\n",
      "[[Pharmacho.sub1 Pharmacho.hasNN Pharmacho.stroke]]\n",
      "[[Pharmacho.s2 Pharmacho.hasaSub Pharmacho.sub2]\n",
      " [Pharmacho.s2 Pharmacho.hasbPre Pharmacho.pre2]\n",
      " [Pharmacho.s2 Pharmacho.hascObj Pharmacho.obj2]]\n",
      "[[Pharmacho.sub2 Pharmacho.hasZnext Pharmacho.fnom1]]\n",
      "[[Pharmacho.fnom1 Pharmacho.hasNN Pharmacho.tekanan]\n",
      " [Pharmacho.fnom1 Pharmacho.hasZnext Pharmacho.fnom2]]\n",
      "[[Pharmacho.fnom2 Pharmacho.hasFnom Pharmacho.fnom2a]\n",
      " [Pharmacho.fnom2 Pharmacho.hasFnom Pharmacho.fnom2b]]\n",
      "[[Pharmacho.fnom2a Pharmacho.hasNN Pharmacho.darah]]\n",
      "[[Pharmacho.fnom2b Pharmacho.hasNNP Pharmacho.tinggi]]\n",
      "[[Pharmacho.pre2 Pharmacho.hasZnext Pharmacho.fverb1]]\n",
      "[[Pharmacho.fverb1 Pharmacho.hasFverb Pharmacho.fverb1a]\n",
      " [Pharmacho.fverb1 Pharmacho.hasFverb Pharmacho.fverb1b]]\n",
      "[[Pharmacho.fverb1a Pharmacho.hasMD Pharmacho.dapat]]\n",
      "[[Pharmacho.fverb1b Pharmacho.hasVB Pharmacho.menyebabkan]]\n",
      "[[Pharmacho.obj2 Pharmacho.hasNN Pharmacho.kematian]]\n",
      "[[Pharmacho.sub2 Pharmacho.hasZnext Pharmacho.fnom1]]\n",
      "[[Pharmacho.fnom1 Pharmacho.hasNN Pharmacho.tekanan]\n",
      " [Pharmacho.fnom1 Pharmacho.hasZnext Pharmacho.fnom2]]\n",
      "[[Pharmacho.fnom2 Pharmacho.hasFnom Pharmacho.fnom2a]\n",
      " [Pharmacho.fnom2 Pharmacho.hasFnom Pharmacho.fnom2b]]\n",
      "[[Pharmacho.fnom2a Pharmacho.hasNN Pharmacho.darah]]\n",
      "[[Pharmacho.fnom2b Pharmacho.hasNNP Pharmacho.tinggi]]\n",
      "[[Pharmacho.s3 Pharmacho.hasaSub Pharmacho.sub3]\n",
      " [Pharmacho.s3 Pharmacho.hasbPre Pharmacho.pre3]\n",
      " [Pharmacho.s3 Pharmacho.hascObj Pharmacho.obj3]]\n",
      "[[Pharmacho.sub3 Pharmacho.hasZnext Pharmacho.fnom3]]\n",
      "[[Pharmacho.fnom3 Pharmacho.hasNN Pharmacho.tekanan]\n",
      " [Pharmacho.fnom3 Pharmacho.hasZnext Pharmacho.fnom4]]\n",
      "[[Pharmacho.fnom4 Pharmacho.hasFnom Pharmacho.fnom4a]\n",
      " [Pharmacho.fnom4 Pharmacho.hasFnom Pharmacho.fnom4b]]\n",
      "[[Pharmacho.fnom4a Pharmacho.hasNN Pharmacho.darah]]\n",
      "[[Pharmacho.fnom4b Pharmacho.hasNN Pharmacho.tinggi]]\n",
      "[[Pharmacho.pre3 Pharmacho.hasZnext Pharmacho.fverb2]]\n",
      "[[Pharmacho.fverb2 Pharmacho.hasFverb Pharmacho.fverb2a]\n",
      " [Pharmacho.fverb2 Pharmacho.hasFverb Pharmacho.fverb2b]]\n",
      "[[Pharmacho.fverb2a Pharmacho.hasMD Pharmacho.dapat]]\n",
      "[[Pharmacho.fverb2b Pharmacho.hasVB Pharmacho.menyebabkan]]\n",
      "[[Pharmacho.obj3 Pharmacho.hasNN Pharmacho.kematian]]\n",
      "[[Pharmacho.sub3 Pharmacho.hasZnext Pharmacho.fnom3]]\n",
      "[[Pharmacho.fnom3 Pharmacho.hasNN Pharmacho.tekanan]\n",
      " [Pharmacho.fnom3 Pharmacho.hasZnext Pharmacho.fnom4]]\n",
      "[[Pharmacho.fnom4 Pharmacho.hasFnom Pharmacho.fnom4a]\n",
      " [Pharmacho.fnom4 Pharmacho.hasFnom Pharmacho.fnom4b]]\n",
      "[[Pharmacho.fnom4a Pharmacho.hasNN Pharmacho.darah]]\n",
      "[[Pharmacho.fnom4b Pharmacho.hasNN Pharmacho.tinggi]]\n"
     ]
    }
   ],
   "source": [
    "#recursion('okbi:s3')\n",
    "res_recursion = []\n",
    "# recursion('okbi:s1')\n",
    "for individual in onto.individuals():\n",
    "    individual_name = individual.name\n",
    "    if individual_name.startswith(\"s\"):\n",
    "        recursion(f\"okbi:{individual_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(res_recursion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = \"\"\n",
    "\n",
    "for i, x in enumerate(res_recursion):\n",
    "    res += str(x)\n",
    "    if(len(res_recursion) == i+1):\n",
    "        continue\n",
    "    res += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"FS1.txt\", 'w') as f:\n",
    "    f.write(str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preproses selesai\n"
     ]
    }
   ],
   "source": [
    "#PREPROSES HASIL QUERY\n",
    "import re\n",
    "\n",
    "inputs = open(r\"FS1.txt\",\"r\").read()\n",
    "inputs = inputs.split(\"\\n\")\n",
    "\n",
    "file1 = open(\"FS2.txt\",\"w\")\n",
    "\n",
    "for x, i in enumerate(inputs):\n",
    "    a = i.replace(\"[\", \"\")\n",
    "    b = a.replace(\"]\", \"\")\n",
    "    b = b.replace(\"'\", \"\")\n",
    "    b = b.replace(\",\", \"\")\n",
    "    c = b.replace(\"Pharmacho.\", \"\")\n",
    "     \n",
    "    file1.writelines (c.strip())\n",
    "\n",
    "    if(len(inputs) == x+1):\n",
    "        continue\n",
    "    file1.writelines (\"\\n\")\n",
    "   \n",
    "print(\"preproses selesai\")\n",
    "file1.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bacaTriplet(namaFile):\n",
    "    lines = []\n",
    "    with open(namaFile,'r+') as f:\n",
    "        line=f.read().splitlines()\n",
    "        lines.append(line)\n",
    "    lines = np.array(lines)\n",
    "    subjek = []\n",
    "    for i in range(lines.shape[1]):\n",
    "        a = lines[0,i].split(\" \")\n",
    "        subjek.append(a)\n",
    "    return subjek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalimat(x, file):\n",
    "    result = []\n",
    "    for i in range(file.shape[0]):\n",
    "        if(file[i,0] == x): \n",
    "            result.append(file[i,2])\n",
    "            if(file[i,1] != 'hasbPre' and file[i,1] != 'hascObj' and \n",
    "               file[i,1] != 'hasdPel' and file[i,1] != 'haseKet' and \n",
    "               file[i,1] != 'hasFnom' and file[i,1] != 'hasFadje' and\n",
    "               file[i,1] != 'hasFverb' and file[i,1] != 'hasFnum' and\n",
    "               file[i,1] != 'hasPewatas' and file[i,1] != 'hasKlausa' and\n",
    "               file[i,1] != 'hasFprep' and file[i,1] != 'hasZnext' and\n",
    "               file[i,1] != 'hasaSub' and file[i,1] != 'hasNext'):\n",
    "                print(file[i,2],end=' ')\n",
    "        \n",
    "            \n",
    "    result = np.array(result)\n",
    "    \n",
    "    for i in result:\n",
    "        kalimat(i, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stroke stroke merupakan penyakit kronis \n",
      "tekanan tekanan darah darah tinggi tinggi darah darah tinggi tinggi darah darah tinggi tinggi darah darah tinggi tinggi tekanan tekanan darah darah tinggi tinggi darah darah tinggi tinggi darah darah tinggi tinggi darah darah tinggi tinggi dapat menyebabkan kematian \n",
      "tekanan tekanan darah darah tinggi tinggi darah darah tinggi tinggi darah darah tinggi tinggi darah darah tinggi tinggi tekanan tekanan darah darah tinggi tinggi darah darah tinggi tinggi darah darah tinggi tinggi darah darah tinggi tinggi dapat menyebabkan kematian \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = bacaTriplet('FS2.txt')\n",
    "file = np.array(file)\n",
    " \n",
    "# kalimat('s1',file)\n",
    "for i in range(1, len(file) + 1):\n",
    "    # stc = sentence\n",
    "    stc = f\"s{i}\" \n",
    "    kalimat(stc, file)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['s1', 'hasaSub', 'sub1'],\n",
       "       ['s1', 'hasbPre', 'pre1'],\n",
       "       ['s1', 'hascObj', 'obj1'],\n",
       "       ['sub1', 'hasNN', 'stroke'],\n",
       "       ['pre1', 'hasVB', 'merupakan'],\n",
       "       ['obj1', 'hasFadje', 'nn'],\n",
       "       ['obj1', 'hasZnext', 'fadje1'],\n",
       "       ['fadje1', 'hasFadje', 'fadje1a'],\n",
       "       ['fadje1', 'hasFadje', 'fadje1b'],\n",
       "       ['fadje1a', 'hasNN', 'penyakit'],\n",
       "       ['fadje1b', 'hasJJ', 'kronis'],\n",
       "       ['sub1', 'hasNN', 'stroke'],\n",
       "       ['s2', 'hasaSub', 'sub2'],\n",
       "       ['s2', 'hasbPre', 'pre2'],\n",
       "       ['s2', 'hascObj', 'obj2'],\n",
       "       ['sub2', 'hasZnext', 'fnom1'],\n",
       "       ['fnom1', 'hasNN', 'tekanan'],\n",
       "       ['fnom1', 'hasZnext', 'fnom2'],\n",
       "       ['fnom2', 'hasFnom', 'fnom2a'],\n",
       "       ['fnom2', 'hasFnom', 'fnom2b'],\n",
       "       ['fnom2a', 'hasNN', 'darah'],\n",
       "       ['fnom2b', 'hasNNP', 'tinggi'],\n",
       "       ['pre2', 'hasZnext', 'fverb1'],\n",
       "       ['fverb1', 'hasFverb', 'fverb1a'],\n",
       "       ['fverb1', 'hasFverb', 'fverb1b'],\n",
       "       ['fverb1a', 'hasMD', 'dapat'],\n",
       "       ['fverb1b', 'hasVB', 'menyebabkan'],\n",
       "       ['obj2', 'hasNN', 'kematian'],\n",
       "       ['sub2', 'hasZnext', 'fnom1'],\n",
       "       ['fnom1', 'hasNN', 'tekanan'],\n",
       "       ['fnom1', 'hasZnext', 'fnom2'],\n",
       "       ['fnom2', 'hasFnom', 'fnom2a'],\n",
       "       ['fnom2', 'hasFnom', 'fnom2b'],\n",
       "       ['fnom2a', 'hasNN', 'darah'],\n",
       "       ['fnom2b', 'hasNNP', 'tinggi'],\n",
       "       ['s3', 'hasaSub', 'sub3'],\n",
       "       ['s3', 'hasbPre', 'pre3'],\n",
       "       ['s3', 'hascObj', 'obj3'],\n",
       "       ['sub3', 'hasZnext', 'fnom3'],\n",
       "       ['fnom3', 'hasNN', 'tekanan'],\n",
       "       ['fnom3', 'hasZnext', 'fnom4'],\n",
       "       ['fnom4', 'hasFnom', 'fnom4a'],\n",
       "       ['fnom4', 'hasFnom', 'fnom4b'],\n",
       "       ['fnom4a', 'hasNN', 'darah'],\n",
       "       ['fnom4b', 'hasNN', 'tinggi'],\n",
       "       ['pre3', 'hasZnext', 'fverb2'],\n",
       "       ['fverb2', 'hasFverb', 'fverb2a'],\n",
       "       ['fverb2', 'hasFverb', 'fverb2b'],\n",
       "       ['fverb2a', 'hasMD', 'dapat'],\n",
       "       ['fverb2b', 'hasVB', 'menyebabkan'],\n",
       "       ['obj3', 'hasNN', 'kematian'],\n",
       "       ['sub3', 'hasZnext', 'fnom3'],\n",
       "       ['fnom3', 'hasNN', 'tekanan'],\n",
       "       ['fnom3', 'hasZnext', 'fnom4'],\n",
       "       ['fnom4', 'hasFnom', 'fnom4a'],\n",
       "       ['fnom4', 'hasFnom', 'fnom4b'],\n",
       "       ['fnom4a', 'hasNN', 'darah'],\n",
       "       ['fnom4b', 'hasNN', 'tinggi']], dtype='<U11')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Questions generated successfully by category\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(triples):\n",
    "    sentences = []\n",
    "    for triple in triples:\n",
    "        subject = triple[0]\n",
    "        predicate = triple[1]\n",
    "        obj = triple[2]\n",
    "        # if predicate == 'hasaSub':\n",
    "        #     predicate = 'adalah subjek dari'\n",
    "        # elif predicate == 'hasbPre':\n",
    "        #     predicate = 'memiliki predikat'\n",
    "        # elif predicate == 'hascObj':\n",
    "        #     predicate = 'memiliki objek'\n",
    "        # elif predicate == 'hasNN':\n",
    "        #     predicate = 'adalah'\n",
    "        # elif predicate == 'hasVB':\n",
    "        #     predicate = 'adalah kata kerja'\n",
    "        # elif predicate == 'hasJJ':\n",
    "        #     predicate = 'adalah kata sifat'\n",
    "        # elif predicate == 'hasCC':\n",
    "        #     predicate = 'adalah konjungsi'\n",
    "        # else:\n",
    "        #     predicate = predicate  # Default case to handle other predicates\n",
    "        sentence = f\"{subject} {predicate} {obj}\"\n",
    "        sentences.append((subject, obj, sentence))\n",
    "    return sentences\n",
    "\n",
    "def read_categories_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    categories = {}\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            cells = row.cells\n",
    "            if len(cells) >= 2:\n",
    "                kategori = cells[1].text.strip()\n",
    "                contoh_soal = cells[2].text.strip()\n",
    "                if kategori not in categories:\n",
    "                    categories[kategori] = []\n",
    "                categories[kategori].append(contoh_soal)\n",
    "    return categories\n",
    "\n",
    "def generate_questions_by_category(sentences, categories):\n",
    "    questions_by_category = {}\n",
    "    for category, examples in categories.items():\n",
    "        questions_by_category[category] = []\n",
    "        for sentence in sentences:\n",
    "            for example in examples:\n",
    "                subject, obj, generated_sentence = sentence\n",
    "                if \"hubungan antara\" in example.lower():\n",
    "                    question = example.replace(\"hubungan antara\", f\"hubungan antara {subject} dan {obj}\")\n",
    "                else:\n",
    "                    question = example\n",
    "                questions_by_category[category].append((question, generated_sentence))\n",
    "    return questions_by_category\n",
    "\n",
    "triples = []\n",
    "for line in file:\n",
    "    triples.append(line)\n",
    "\n",
    "sentences = generate_sentence(triples)\n",
    "\n",
    "# Read categories and examples from docx file\n",
    "categories = read_categories_from_docx('Kategori_dan_contoh_soal.docx')\n",
    "    \n",
    "questions_by_category = generate_questions_by_category(sentences, categories)\n",
    "\n",
    "with open(\"Questions_by_category.txt\", 'w') as f:\n",
    "    for category, questions in questions_by_category.items():\n",
    "        f.write(f\"Kategori: {category}\\n\")\n",
    "        for question, answer in questions:\n",
    "            f.write(f\"Q: {question}\\n\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "logging.info(\"Questions generated successfully by category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology loaded.\n",
      "All individuals have been deleted and ontology has been saved.\n"
     ]
    }
   ],
   "source": [
    "# menghapus individual untuk re-generate soal (gunakan ketika soal sudah tergenerate)\n",
    "\n",
    "from owlready2 import get_ontology\n",
    "\n",
    "# Memuat ontologi dari file\n",
    "ontology_path = \"Pharmacho.owl\"\n",
    "ontology = get_ontology(ontology_path).load()\n",
    "print(\"Ontology loaded.\")\n",
    "\n",
    "# Menghapus semua individu dari ontologi\n",
    "with ontology:\n",
    "    for individual in list(ontology.individuals()):\n",
    "        destroy_entity(individual)\n",
    "\n",
    "# Menyimpan ontologi yang telah diperbarui\n",
    "ontology.save(file=ontology_path)\n",
    "print(\"All individuals have been deleted and ontology has been saved.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
